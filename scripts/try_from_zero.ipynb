{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5559983705040591)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_parquet(\"..\\data\\\\train.parquet\")\n",
    "test = pd.read_parquet(\"..\\data\\\\final_test.parquet\")\n",
    "\n",
    "# We drop bike count, as it can't be used for training\n",
    "train.drop(columns=[\"bike_count\"], inplace=True)\n",
    "\n",
    "X = train.drop(columns=[\"log_bike_count\"])\n",
    "y = train[\"log_bike_count\"]\n",
    "\n",
    "\n",
    "def train_test_split_temporal(X, y, delta_threshold=\"30 days\"):\n",
    "    \n",
    "    cutoff_date = X[\"date\"].max() - pd.Timedelta(delta_threshold)\n",
    "    mask = (X[\"date\"] <= cutoff_date)\n",
    "    X_train, X_valid = X.loc[mask], X.loc[~mask]\n",
    "    y_train, y_valid = y[mask], y[~mask]\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "X_train, y_train, X_valid, y_valid = train_test_split_temporal(X, y)\n",
    "\n",
    "def kaggle_prediction(model, file_path, test_set=test):\n",
    "    \"\"\"\n",
    "    Return a .csv file for kaggle submission (predictions of the test dataset)\n",
    "    Parameters:\n",
    "        - model : a fitted sklearn model object\n",
    "        - test_set : the dataset to predict log_bike_count on \n",
    "    Output:\n",
    "        - file.csv : a .csv file to submit to kaggle \n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = model.predict(test_set)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=[\"log_bike_count\"])\n",
    "    y_pred_df.index.name = \"Id\"\n",
    "\n",
    "    y_pred_df.to_csv(file_path)\n",
    "\n",
    "    return None\n",
    "\n",
    "columns_to_drop =  [\"counter_name\", \"counter_id\", \"site_name\", \"site_id\", \"coordinates\", \"counter_technical_id\", \"counter_installation_date\"]\n",
    "\n",
    "def drop_columns(X, columns_to_drop=columns_to_drop):\n",
    "    return X.drop(columns=columns_to_drop)\n",
    "\n",
    "# column_dropper = FunctionTransformer(lambda X: drop_columns(X, columns_to_drop=columns_to_drop), validate=False)\n",
    "column_dropper = FunctionTransformer(drop_columns, validate=False)\n",
    "\n",
    "\n",
    "def encode_dates(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    # Encode the date information from the \"date\" columns\n",
    "    X[\"year\"] = X[\"date\"].dt.year\n",
    "    X[\"month\"] = X[\"date\"].dt.month\n",
    "    X[\"day\"] = X[\"date\"].dt.day\n",
    "    X[\"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X[\"hour\"] = X[\"date\"].dt.hour\n",
    "\n",
    "    # Finally we can drop the original columns from the dataframe\n",
    "    return X.drop(columns=[\"date\"])\n",
    "\n",
    "date_encoder = FunctionTransformer(encode_dates, validate=False)\n",
    "date_cols = encode_dates(X[[\"date\"]]).columns.tolist()\n",
    "\n",
    "\n",
    "vacation_paris_2020 = [\n",
    "    (\"2020-10-17\", \"2020-11-01\"),  \n",
    "    (\"2020-12-19\", \"2021-01-03\"),  \n",
    "    (\"2021-02-13\", \"2021-02-28\"),  \n",
    "    (\"2021-04-17\", \"2021-05-02\"), \n",
    "    (\"2021-07-06\", \"2021-08-31\"), \n",
    "]\n",
    "\n",
    "vacation_paris_2020 = [(pd.to_datetime(start), pd.to_datetime(end)) for start, end in vacation_paris_2020]\n",
    "\n",
    "def encode_vacation(X, vacation=vacation_paris_2020):\n",
    "    X = X.copy()\n",
    "    X[\"is_vacation\"] = 0\n",
    "    for start, end in vacation:\n",
    "        X[\"is_vacation\"] |= (X[\"date\"] >= start) & (X[\"date\"] <= end)\n",
    "    X[\"is_vacation\"] = X[\"is_vacation\"].astype(int)\n",
    "\n",
    "    return X\n",
    "\n",
    "vacation_encoder = FunctionTransformer(encode_vacation, validate=False)\n",
    "\n",
    "\n",
    "holidays = [\n",
    "    \"2020-11-01\",  \n",
    "    \"2020-11-11\", \n",
    "    \"2020-12-25\",  \n",
    "    \"2021-01-01\",  \n",
    "    \"2021-04-05\",  \n",
    "    \"2021-05-01\",  \n",
    "    \"2021-05-08\",  \n",
    "    \"2021-05-13\", \n",
    "    \"2021-05-24\",  \n",
    "    \"2021-07-14\",  \n",
    "    \"2021-08-15\", \n",
    "]\n",
    "holidays = pd.to_datetime(holidays)\n",
    "\n",
    "\n",
    "def encode_holidays(X, holidays=holidays):\n",
    "    X = X.copy()\n",
    "    X[\"jour_ferie\"] = X[\"date\"].apply(lambda x: 1 if x in holidays else 0)\n",
    "    return X\n",
    "\n",
    "holidays_encoder = FunctionTransformer(encode_holidays, validate=False)\n",
    "\n",
    "categorical_columns = drop_columns(X).select_dtypes(include=\"category\").columns.to_list()\n",
    "categorical_columns.append(\"site_id\")\n",
    "numerical_columns = drop_columns(X).select_dtypes(include=\"float\").columns.to_list()\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def compute_score(estimator, X_train, X_valid, y_train, y_valid):\n",
    "    estimator.fit(X_train, y_train)\n",
    "    score = root_mean_squared_error(y_valid, estimator.predict(X_valid))\n",
    "    return score\n",
    "\n",
    "\n",
    "weather = pd.read_csv(\"../data/H_75_previous-2020-2022.csv.gz\", sep=\";\",\n",
    "                      parse_dates=[\"AAAAMMJJHH\"],\n",
    "                      date_format=\"%Y%m%d%H\",\n",
    "                      ).rename(columns={\"AAAAMMJJHH\" : \"date\"})\n",
    "\n",
    "\n",
    "weather_columns = [\"U\", \"FF\", \"RR1\", \"T\"]\n",
    "columns_to_merge = weather_columns + [\"date\"]\n",
    "\n",
    "def merge_weather(X, weather=weather, columns_to_merge=columns_to_merge):\n",
    "    weather = weather.copy()\n",
    "    weather = weather[columns_to_merge].sort_values(\"date\")\n",
    "    weather[\"date\"] = weather[\"date\"].astype(\"datetime64[us]\")\n",
    "\n",
    "    start = pd.Timestamp(min(X[\"date\"]))\n",
    "    end = pd.Timestamp(max(X[\"date\"]))\n",
    "\n",
    "    weather = weather[(weather[\"date\"] >= start) & (weather[\"date\"] <= end)]\n",
    "\n",
    "    grouped_weather = weather.groupby(\"date\").mean()\n",
    "\n",
    "    X = X.copy()\n",
    "    initial_index = X.index  # Save original index\n",
    "    X = X.sort_values(\"date\")\n",
    "\n",
    "    # Perform the merge_asof\n",
    "    merged_df = pd.merge_asof(X, grouped_weather, on=\"date\", direction=\"nearest\")\n",
    "\n",
    "    # Reindex to ensure alignment with X\n",
    "    merged_df.index = initial_index  # Reassign original index\n",
    "    merged_df = merged_df.reindex(initial_index)  # Align rows to original order\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "weather_merger = FunctionTransformer(merge_weather, validate=False)\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"date\", OneHotEncoder(handle_unknown=\"ignore\"), date_cols),\n",
    "        # (\"weather numerical\", StandardScaler(), numerical_weather_columns),\n",
    "        # (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"dropper\", column_dropper),\n",
    "        (\"vacation\", vacation_encoder),\n",
    "        (\"date\", date_encoder),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"linear_regressor\", XGBRegressor(n_estimators=350, \n",
    "                                          learning_rate=0.1, \n",
    "                                          max_depth=20, \n",
    "                                          random_state=42, \n",
    "                                          objective='reg:squarederror'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "compute_score(pipeline, X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_prediction(model, file_path, test_set=test):\n",
    "    \"\"\"\n",
    "    Return a .csv file for kaggle submission (predictions of the test dataset)\n",
    "    Parameters:\n",
    "        - model : a fitted sklearn model object\n",
    "        - test_set : the dataset to predict log_bike_count on \n",
    "    Output:\n",
    "        - file.csv : a .csv file to submit to kaggle \n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = model.predict(test_set)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=[\"log_bike_count\"])\n",
    "    y_pred_df.index.name = \"Id\"\n",
    "\n",
    "    y_pred_df.to_csv(file_path)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_prediction(pipeline.fit(X, y), \"../prediction_csvs/test_kaggle.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5432520928059277)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_parquet(\"..\\data\\\\train.parquet\")\n",
    "test = pd.read_parquet(\"..\\data\\\\final_test.parquet\")\n",
    "\n",
    "# We drop bike count, as it can't be used for training\n",
    "train.drop(columns=[\"bike_count\"], inplace=True)\n",
    "\n",
    "X = train.drop(columns=[\"log_bike_count\"])\n",
    "y = train[\"log_bike_count\"]\n",
    "\n",
    "\n",
    "def train_test_split_temporal(X, y, delta_threshold=\"30 days\"):\n",
    "    \n",
    "    cutoff_date = X[\"date\"].max() - pd.Timedelta(delta_threshold)\n",
    "    mask = (X[\"date\"] <= cutoff_date)\n",
    "    X_train, X_valid = X.loc[mask], X.loc[~mask]\n",
    "    y_train, y_valid = y[mask], y[~mask]\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "X_train, y_train, X_valid, y_valid = train_test_split_temporal(X, y)\n",
    "\n",
    "def kaggle_prediction(model, file_path, test_set=test):\n",
    "    \"\"\"\n",
    "    Return a .csv file for kaggle submission (predictions of the test dataset)\n",
    "    Parameters:\n",
    "        - model : a fitted sklearn model object\n",
    "        - test_set : the dataset to predict log_bike_count on \n",
    "    Output:\n",
    "        - file.csv : a .csv file to submit to kaggle \n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = model.predict(test_set)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=[\"log_bike_count\"])\n",
    "    y_pred_df.index.name = \"Id\"\n",
    "\n",
    "    y_pred_df.to_csv(file_path)\n",
    "\n",
    "    return None\n",
    "\n",
    "columns_to_drop =  [\"counter_name\", \"counter_id\", \"site_name\", \"site_id\", \"coordinates\", \"counter_technical_id\", \"counter_installation_date\"]\n",
    "\n",
    "def drop_columns(X, columns_to_drop=columns_to_drop):\n",
    "    return X.drop(columns=columns_to_drop)\n",
    "\n",
    "# column_dropper = FunctionTransformer(lambda X: drop_columns(X, columns_to_drop=columns_to_drop), validate=False)\n",
    "column_dropper = FunctionTransformer(drop_columns, validate=False)\n",
    "\n",
    "\n",
    "def encode_dates(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    # Encode the date information from the \"date\" columns\n",
    "    X[\"year\"] = X[\"date\"].dt.year\n",
    "    X[\"month\"] = X[\"date\"].dt.month\n",
    "    X[\"day\"] = X[\"date\"].dt.day\n",
    "    X[\"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X[\"hour\"] = X[\"date\"].dt.hour\n",
    "\n",
    "    # Finally we can drop the original columns from the dataframe\n",
    "    return X.drop(columns=[\"date\"])\n",
    "\n",
    "date_encoder = FunctionTransformer(encode_dates, validate=False)\n",
    "date_cols = encode_dates(X[[\"date\"]]).columns.tolist()\n",
    "\n",
    "\n",
    "vacation_paris_2020 = [\n",
    "    (\"2020-10-17\", \"2020-11-01\"),  \n",
    "    (\"2020-12-19\", \"2021-01-03\"),  \n",
    "    (\"2021-02-13\", \"2021-02-28\"),  \n",
    "    (\"2021-04-17\", \"2021-05-02\"), \n",
    "    (\"2021-07-06\", \"2021-08-31\"), \n",
    "]\n",
    "\n",
    "vacation_paris_2020 = [(pd.to_datetime(start), pd.to_datetime(end)) for start, end in vacation_paris_2020]\n",
    "\n",
    "def encode_vacation(X, vacation=vacation_paris_2020):\n",
    "    X = X.copy()\n",
    "    X[\"is_vacation\"] = 0\n",
    "    for start, end in vacation:\n",
    "        X[\"is_vacation\"] |= (X[\"date\"] >= start) & (X[\"date\"] <= end)\n",
    "    X[\"is_vacation\"] = X[\"is_vacation\"].astype(int)\n",
    "\n",
    "    return X\n",
    "\n",
    "vacation_encoder = FunctionTransformer(encode_vacation, validate=False)\n",
    "\n",
    "\n",
    "holidays = [\n",
    "    \"2020-11-01\",  \n",
    "    \"2020-11-11\", \n",
    "    \"2020-12-25\",  \n",
    "    \"2021-01-01\",  \n",
    "    \"2021-04-05\",  \n",
    "    \"2021-05-01\",  \n",
    "    \"2021-05-08\",  \n",
    "    \"2021-05-13\", \n",
    "    \"2021-05-24\",  \n",
    "    \"2021-07-14\",  \n",
    "    \"2021-08-15\", \n",
    "]\n",
    "holidays = pd.to_datetime(holidays)\n",
    "\n",
    "\n",
    "lockdown = [\n",
    "    (\"2020-10-30\", \"2020-12-15\", \"lockdown\"),\n",
    "    (\"2021-03-20\", \"2021-04-17\", \"lockdown\"), \n",
    "]\n",
    "\n",
    "curfew = [\n",
    "    (\"2020-10-17\", \"2020-12-14\", \"21:00\", \"06:00\", \"curfew\"),\n",
    "    (\"2020-12-15\", \"2021-01-15\", \"20:00\", \"06:00\", \"curfew\"),\n",
    "    (\"2021-01-16\", \"2021-03-19\", \"18:00\", \"06:00\", \"curfew\"),\n",
    "    (\"2021-03-20\", \"2021-06-08\", \"19:00\", \"06:00\", \"curfew\"),\n",
    "    (\"2021-06-09\", \"2021-06-19\", \"23:00\", \"06:00\", \"curfew\"),\n",
    "]\n",
    "\n",
    "lockdown = [(pd.to_datetime(start), pd.to_datetime(end), label) for start, end, label in lockdown]\n",
    "curfew = [(pd.to_datetime(start), pd.to_datetime(end), start_hour, end_hour, label) \n",
    "          for start, end, start_hour, end_hour, label in curfew]\n",
    "\n",
    "def encode_lockdown_curfew(X, lockdown=lockdown, curfew=curfew):\n",
    "    X = X.copy()\n",
    "    X[\"is_lockdown\"] = 0\n",
    "    X[\"is_curfew\"] = 0\n",
    "\n",
    "    for start, end, label in lockdown:\n",
    "        X[\"is_lockdown\"] |= (X[\"date\"] >= start) & (X[\"date\"] <= end)\n",
    "\n",
    "    for start, end, start_hour, end_hour, label in curfew:\n",
    "        X[\"is_curfew\"] |= ((X[\"date\"] >= start) & (X[\"date\"] <= end) & \n",
    "                           ((X[\"date\"].dt.time >= pd.to_datetime(start_hour).time()) | \n",
    "                            (X[\"date\"].dt.time <= pd.to_datetime(end_hour).time())))\n",
    "    return X\n",
    "\n",
    "lockdown_curfew_encoder = FunctionTransformer(encode_lockdown_curfew, validate=False)\n",
    "\n",
    "def encode_holidays(X, holidays=holidays):\n",
    "    X = X.copy()\n",
    "    X[\"jour_ferie\"] = X[\"date\"].apply(lambda x: 1 if x in holidays else 0)\n",
    "    return X\n",
    "\n",
    "holidays_encoder = FunctionTransformer(encode_holidays, validate=False)\n",
    "\n",
    "categorical_columns = drop_columns(X).select_dtypes(include=\"category\").columns.to_list()\n",
    "categorical_columns.append(\"site_id\")\n",
    "numerical_columns = drop_columns(X).select_dtypes(include=\"float\").columns.to_list()\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def compute_score(estimator, X_train, X_valid, y_train, y_valid):\n",
    "    estimator.fit(X_train, y_train)\n",
    "    score = root_mean_squared_error(y_valid, estimator.predict(X_valid))\n",
    "    return score\n",
    "\n",
    "\n",
    "weather = pd.read_csv(\"../external_data/external_data.csv\")\n",
    "\n",
    "\n",
    "weather_columns = [\"u\", \"ff\", \"rr1\", \"t\"]\n",
    "columns_to_merge = weather_columns + [\"date\"]\n",
    "\n",
    "\n",
    "def merge_weather(X, weather=weather, columns_to_merge=columns_to_merge):\n",
    "    weather = weather.copy()\n",
    "    weather = weather[columns_to_merge]\n",
    "    weather[\"date\"] = weather[\"date\"].astype(\"datetime64[us]\")\n",
    "    \n",
    "    grouped_weather = weather.groupby(\"date\").mean()\n",
    "\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Perform the merge while keeping the left index\n",
    "    merged_df = pd.merge(X, grouped_weather, on=\"date\", how=\"left\")\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weather_merger = FunctionTransformer(merge_weather, validate=False)\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"date\", OneHotEncoder(handle_unknown=\"ignore\"), date_cols),\n",
    "        # (\"weather numerical\", StandardScaler(), numerical_weather_columns),\n",
    "        # (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"dropper\", column_dropper),\n",
    "        (\"vacation\", vacation_encoder),\n",
    "        (\"holiday\", holidays_encoder),\n",
    "        (\"lockdown curfew\", lockdown_curfew_encoder),\n",
    "        (\"weather merge\", weather_merger),\n",
    "        (\"date\", date_encoder),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"linear_regressor\", XGBRegressor(n_estimators=350, \n",
    "                                          learning_rate=0.1, \n",
    "                                          max_depth=20, \n",
    "                                          random_state=42, \n",
    "                                          objective='reg:squarederror'))\n",
    "    ]\n",
    ")\n",
    "compute_score(pipeline, X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-11 22:22:54,867] A new study created in memory with name: no-name-5854b32a-f35d-4f38-9bd0-95fd1db73356\n",
      "[I 2024-12-11 22:23:43,428] Trial 0 finished with value: 0.5970131368965089 and parameters: {'n_estimators': 218, 'learning_rate': 0.27581786863679475, 'max_depth': 12, 'subsample': 0.7539848818038062}. Best is trial 0 with value: 0.5970131368965089.\n",
      "[I 2024-12-11 22:25:37,323] Trial 1 finished with value: 0.5579666452834916 and parameters: {'n_estimators': 154, 'learning_rate': 0.13241939173207412, 'max_depth': 19, 'subsample': 0.7215656469779265}. Best is trial 1 with value: 0.5579666452834916.\n",
      "[I 2024-12-11 22:26:15,487] Trial 2 finished with value: 0.5877736499074718 and parameters: {'n_estimators': 126, 'learning_rate': 0.45839134999175607, 'max_depth': 8, 'subsample': 0.6918605172191894}. Best is trial 1 with value: 0.5579666452834916.\n",
      "[I 2024-12-11 22:27:05,119] Trial 3 finished with value: 0.568361267272805 and parameters: {'n_estimators': 242, 'learning_rate': 0.18049849299406787, 'max_depth': 5, 'subsample': 0.9056235828991289}. Best is trial 1 with value: 0.5579666452834916.\n",
      "[I 2024-12-11 22:27:45,896] Trial 4 finished with value: 0.9462288165094854 and parameters: {'n_estimators': 118, 'learning_rate': 0.025745689173598103, 'max_depth': 4, 'subsample': 0.9510291930612417}. Best is trial 1 with value: 0.5579666452834916.\n",
      "[I 2024-12-11 22:28:03,541] Trial 5 finished with value: 0.5488571202040874 and parameters: {'n_estimators': 139, 'learning_rate': 0.12282523374770488, 'max_depth': 9, 'subsample': 0.759661800840508}. Best is trial 5 with value: 0.5488571202040874.\n",
      "[I 2024-12-11 22:28:54,017] Trial 6 finished with value: 0.5528707903715849 and parameters: {'n_estimators': 234, 'learning_rate': 0.29521677013790126, 'max_depth': 17, 'subsample': 0.9996101507393822}. Best is trial 5 with value: 0.5488571202040874.\n",
      "[I 2024-12-11 22:29:34,810] Trial 7 finished with value: 0.5625213564398431 and parameters: {'n_estimators': 193, 'learning_rate': 0.2729240678683808, 'max_depth': 13, 'subsample': 0.8270404338496067}. Best is trial 5 with value: 0.5488571202040874.\n",
      "[I 2024-12-11 22:30:29,731] Trial 8 finished with value: 0.5413639486546936 and parameters: {'n_estimators': 140, 'learning_rate': 0.11496091894139528, 'max_depth': 12, 'subsample': 0.8319941342098728}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:32:01,998] Trial 9 finished with value: 0.5458595126968606 and parameters: {'n_estimators': 175, 'learning_rate': 0.06036455108284606, 'max_depth': 18, 'subsample': 0.7081686830873368}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:33:00,415] Trial 10 finished with value: 0.8470113491605239 and parameters: {'n_estimators': 102, 'learning_rate': 0.010079664430077774, 'max_depth': 15, 'subsample': 0.844623743463013}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:34:32,622] Trial 11 finished with value: 0.5418937333756325 and parameters: {'n_estimators': 174, 'learning_rate': 0.05343803628942139, 'max_depth': 19, 'subsample': 0.6142695797795146}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:35:41,310] Trial 12 finished with value: 0.545386740209236 and parameters: {'n_estimators': 168, 'learning_rate': 0.0521563607913575, 'max_depth': 20, 'subsample': 0.6020390223540202}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:36:32,919] Trial 13 finished with value: 0.5628552542699653 and parameters: {'n_estimators': 201, 'learning_rate': 0.034797682434744834, 'max_depth': 15, 'subsample': 0.6590935555016753}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:37:21,291] Trial 14 finished with value: 0.564489912476826 and parameters: {'n_estimators': 153, 'learning_rate': 0.07934313785729137, 'max_depth': 9, 'subsample': 0.8866551297995507}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:38:40,036] Trial 15 finished with value: 0.6067311321047358 and parameters: {'n_estimators': 190, 'learning_rate': 0.013508208115067567, 'max_depth': 15, 'subsample': 0.6070270481804199}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:39:28,585] Trial 16 finished with value: 0.7117461642175622 and parameters: {'n_estimators': 153, 'learning_rate': 0.026972441909233025, 'max_depth': 7, 'subsample': 0.7980521739756572}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:39:51,620] Trial 17 finished with value: 0.5440577930727711 and parameters: {'n_estimators': 173, 'learning_rate': 0.09339891517033923, 'max_depth': 11, 'subsample': 0.8649618652427267}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:40:32,052] Trial 18 finished with value: 0.5432722792802399 and parameters: {'n_estimators': 130, 'learning_rate': 0.04219906950819893, 'max_depth': 17, 'subsample': 0.7888201342119786}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:40:53,752] Trial 19 finished with value: 0.6750834729539041 and parameters: {'n_estimators': 110, 'learning_rate': 0.018695521329877833, 'max_depth': 13, 'subsample': 0.6558888862897724}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:41:09,951] Trial 20 finished with value: 0.5767327992822975 and parameters: {'n_estimators': 139, 'learning_rate': 0.10231786166453491, 'max_depth': 6, 'subsample': 0.9163297536267787}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:41:50,903] Trial 21 finished with value: 0.5414958864842621 and parameters: {'n_estimators': 131, 'learning_rate': 0.045277076947207576, 'max_depth': 17, 'subsample': 0.7970066355774802}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:43:18,254] Trial 22 finished with value: 0.5420973025969126 and parameters: {'n_estimators': 139, 'learning_rate': 0.053105446507011024, 'max_depth': 20, 'subsample': 0.8260711024816103}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:44:44,423] Trial 23 finished with value: 0.5517741320192899 and parameters: {'n_estimators': 154, 'learning_rate': 0.17051894610754237, 'max_depth': 17, 'subsample': 0.7570538549912126}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:45:59,006] Trial 24 finished with value: 0.5516502293755127 and parameters: {'n_estimators': 164, 'learning_rate': 0.06610446862632584, 'max_depth': 15, 'subsample': 0.948191375477468}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:46:36,841] Trial 25 finished with value: 0.6169666377916279 and parameters: {'n_estimators': 122, 'learning_rate': 0.02558583012309085, 'max_depth': 11, 'subsample': 0.6463237086911007}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:47:39,921] Trial 26 finished with value: 0.557529699773846 and parameters: {'n_estimators': 211, 'learning_rate': 0.03653107922647081, 'max_depth': 18, 'subsample': 0.8622989175388571}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:48:18,704] Trial 27 finished with value: 0.546711931675902 and parameters: {'n_estimators': 182, 'learning_rate': 0.0772584009929824, 'max_depth': 16, 'subsample': 0.804091880241707}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:48:42,345] Trial 28 finished with value: 0.5447378441051216 and parameters: {'n_estimators': 137, 'learning_rate': 0.04508366763845467, 'max_depth': 13, 'subsample': 0.7393641445196452}. Best is trial 8 with value: 0.5413639486546936.\n",
      "[I 2024-12-11 22:49:08,067] Trial 29 finished with value: 0.5401858339357622 and parameters: {'n_estimators': 227, 'learning_rate': 0.12188041018505529, 'max_depth': 12, 'subsample': 0.7756530821336427}. Best is trial 29 with value: 0.5401858339357622.\n",
      "[I 2024-12-11 22:49:29,302] Trial 30 finished with value: 0.5447806645031176 and parameters: {'n_estimators': 225, 'learning_rate': 0.2054746089267065, 'max_depth': 10, 'subsample': 0.7777788544073321}. Best is trial 29 with value: 0.5401858339357622.\n",
      "[I 2024-12-11 22:49:57,166] Trial 31 finished with value: 0.5460779459782872 and parameters: {'n_estimators': 213, 'learning_rate': 0.1244475838285843, 'max_depth': 13, 'subsample': 0.8147883977264483}. Best is trial 29 with value: 0.5401858339357622.\n",
      "[I 2024-12-11 22:50:52,476] Trial 32 finished with value: 0.5517670521522019 and parameters: {'n_estimators': 161, 'learning_rate': 0.09389632812538821, 'max_depth': 19, 'subsample': 0.6798302684632767}. Best is trial 29 with value: 0.5401858339357622.\n",
      "[I 2024-12-11 22:51:25,762] Trial 33 finished with value: 0.5376843569223978 and parameters: {'n_estimators': 184, 'learning_rate': 0.06975456527736798, 'max_depth': 12, 'subsample': 0.7286524122447068}. Best is trial 33 with value: 0.5376843569223978.\n",
      "[I 2024-12-11 22:52:09,216] Trial 34 finished with value: 0.5447408094444588 and parameters: {'n_estimators': 250, 'learning_rate': 0.16075618117077323, 'max_depth': 12, 'subsample': 0.7282554383305271}. Best is trial 33 with value: 0.5376843569223978.\n",
      "[I 2024-12-11 22:52:29,616] Trial 35 finished with value: 0.5392511133505977 and parameters: {'n_estimators': 204, 'learning_rate': 0.13757766876588848, 'max_depth': 10, 'subsample': 0.7002008896110319}. Best is trial 33 with value: 0.5376843569223978.\n",
      "[I 2024-12-11 22:52:53,094] Trial 36 finished with value: 0.552190704508083 and parameters: {'n_estimators': 223, 'learning_rate': 0.25163014724201216, 'max_depth': 10, 'subsample': 0.688646381194733}. Best is trial 33 with value: 0.5376843569223978.\n",
      "[I 2024-12-11 22:53:13,195] Trial 37 finished with value: 0.5678153513312427 and parameters: {'n_estimators': 203, 'learning_rate': 0.3579364634451576, 'max_depth': 8, 'subsample': 0.7224721673586403}. Best is trial 33 with value: 0.5376843569223978.\n",
      "[I 2024-12-11 22:53:38,710] Trial 38 finished with value: 0.5507530655236178 and parameters: {'n_estimators': 233, 'learning_rate': 0.13338011203263278, 'max_depth': 12, 'subsample': 0.773028660028879}. Best is trial 33 with value: 0.5376843569223978.\n",
      "[I 2024-12-11 22:53:58,459] Trial 39 finished with value: 0.546870970859438 and parameters: {'n_estimators': 194, 'learning_rate': 0.201748903811086, 'max_depth': 10, 'subsample': 0.7500199819289565}. Best is trial 33 with value: 0.5376843569223978.\n",
      "[I 2024-12-11 22:54:27,092] Trial 40 finished with value: 0.551942348330456 and parameters: {'n_estimators': 182, 'learning_rate': 0.14246308043853667, 'max_depth': 14, 'subsample': 0.706030720528112}. Best is trial 33 with value: 0.5376843569223978.\n",
      "[I 2024-12-11 22:54:46,847] Trial 41 finished with value: 0.5642886545206588 and parameters: {'n_estimators': 212, 'learning_rate': 0.10614836727836158, 'max_depth': 9, 'subsample': 0.7683672693048045}. Best is trial 33 with value: 0.5376843569223978.\n",
      "[I 2024-12-11 22:55:08,963] Trial 42 finished with value: 0.5374323737361055 and parameters: {'n_estimators': 203, 'learning_rate': 0.07814257788803206, 'max_depth': 11, 'subsample': 0.8420136321973922}. Best is trial 42 with value: 0.5374323737361055.\n",
      "[I 2024-12-11 22:55:31,107] Trial 43 finished with value: 0.5395964765309257 and parameters: {'n_estimators': 203, 'learning_rate': 0.0801541564618324, 'max_depth': 11, 'subsample': 0.8636666945385625}. Best is trial 42 with value: 0.5374323737361055.\n",
      "[I 2024-12-11 22:55:47,032] Trial 44 finished with value: 0.69345002527808 and parameters: {'n_estimators': 201, 'learning_rate': 0.08346685708901314, 'max_depth': 3, 'subsample': 0.8477083176855092}. Best is trial 42 with value: 0.5374323737361055.\n",
      "[I 2024-12-11 22:56:08,676] Trial 45 finished with value: 0.5420673919566875 and parameters: {'n_estimators': 184, 'learning_rate': 0.0669280113840249, 'max_depth': 11, 'subsample': 0.8904420885827216}. Best is trial 42 with value: 0.5374323737361055.\n",
      "[I 2024-12-11 22:56:30,147] Trial 46 finished with value: 0.5575161753948639 and parameters: {'n_estimators': 221, 'learning_rate': 0.06069894061505908, 'max_depth': 10, 'subsample': 0.705920512655256}. Best is trial 42 with value: 0.5374323737361055.\n",
      "[I 2024-12-11 22:56:49,180] Trial 47 finished with value: 0.5700708345467214 and parameters: {'n_estimators': 231, 'learning_rate': 0.15066991700561885, 'max_depth': 8, 'subsample': 0.9207789181578652}. Best is trial 42 with value: 0.5374323737361055.\n",
      "[I 2024-12-11 22:57:22,200] Trial 48 finished with value: 0.5488318470898041 and parameters: {'n_estimators': 206, 'learning_rate': 0.11095404371355075, 'max_depth': 14, 'subsample': 0.7405270316801241}. Best is trial 42 with value: 0.5374323737361055.\n",
      "[I 2024-12-11 22:57:41,942] Trial 49 finished with value: 0.5578063014392716 and parameters: {'n_estimators': 196, 'learning_rate': 0.08229461175374392, 'max_depth': 9, 'subsample': 0.6311673070456705}. Best is trial 42 with value: 0.5374323737361055.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 203, 'learning_rate': 0.07814257788803206, 'max_depth': 11, 'subsample': 0.8420136321973922}\n",
      "Best RMSE: 0.5374323737361055\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna study to optimize XGBRegressor hyperparameters.\n",
    "    \"\"\"\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 250)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.5, log=True)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"dropper\", column_dropper),\n",
    "            (\"vacation\", vacation_encoder),\n",
    "            (\"holiday\", holidays_encoder),\n",
    "            (\"lockdown curfew\", lockdown_curfew_encoder),\n",
    "            (\"weather merge\", weather_merger),\n",
    "            (\"date\", date_encoder),\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"xgb\", XGBRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                learning_rate=learning_rate,\n",
    "                max_depth=max_depth,\n",
    "                subsample=subsample,\n",
    "                objective='reg:squarederror'\n",
    "            ))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_valid)\n",
    "    rmse = sqrt(mean_squared_error(y_valid, y_pred))\n",
    "\n",
    "    return rmse\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best RMSE:\", study.best_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_classes-9Axu7pbz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
