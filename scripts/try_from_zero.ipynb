{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5559983705040591)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_parquet(\"..\\data\\\\train.parquet\")\n",
    "test = pd.read_parquet(\"..\\data\\\\final_test.parquet\")\n",
    "\n",
    "# We drop bike count, as it can't be used for training\n",
    "train.drop(columns=[\"bike_count\"], inplace=True)\n",
    "\n",
    "X = train.drop(columns=[\"log_bike_count\"])\n",
    "y = train[\"log_bike_count\"]\n",
    "\n",
    "\n",
    "def train_test_split_temporal(X, y, delta_threshold=\"30 days\"):\n",
    "    \n",
    "    cutoff_date = X[\"date\"].max() - pd.Timedelta(delta_threshold)\n",
    "    mask = (X[\"date\"] <= cutoff_date)\n",
    "    X_train, X_valid = X.loc[mask], X.loc[~mask]\n",
    "    y_train, y_valid = y[mask], y[~mask]\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "X_train, y_train, X_valid, y_valid = train_test_split_temporal(X, y)\n",
    "\n",
    "def kaggle_prediction(model, file_path, test_set=test):\n",
    "    \"\"\"\n",
    "    Return a .csv file for kaggle submission (predictions of the test dataset)\n",
    "    Parameters:\n",
    "        - model : a fitted sklearn model object\n",
    "        - test_set : the dataset to predict log_bike_count on \n",
    "    Output:\n",
    "        - file.csv : a .csv file to submit to kaggle \n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = model.predict(test_set)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=[\"log_bike_count\"])\n",
    "    y_pred_df.index.name = \"Id\"\n",
    "\n",
    "    y_pred_df.to_csv(file_path)\n",
    "\n",
    "    return None\n",
    "\n",
    "columns_to_drop =  [\"counter_name\", \"counter_id\", \"site_name\", \"site_id\", \"coordinates\", \"counter_technical_id\", \"counter_installation_date\"]\n",
    "\n",
    "def drop_columns(X, columns_to_drop=columns_to_drop):\n",
    "    return X.drop(columns=columns_to_drop)\n",
    "\n",
    "# column_dropper = FunctionTransformer(lambda X: drop_columns(X, columns_to_drop=columns_to_drop), validate=False)\n",
    "column_dropper = FunctionTransformer(drop_columns, validate=False)\n",
    "\n",
    "\n",
    "def encode_dates(X):\n",
    "    X = X.copy()  # modify a copy of X\n",
    "    # Encode the date information from the \"date\" columns\n",
    "    X[\"year\"] = X[\"date\"].dt.year\n",
    "    X[\"month\"] = X[\"date\"].dt.month\n",
    "    X[\"day\"] = X[\"date\"].dt.day\n",
    "    X[\"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X[\"hour\"] = X[\"date\"].dt.hour\n",
    "\n",
    "    # Finally we can drop the original columns from the dataframe\n",
    "    return X.drop(columns=[\"date\"])\n",
    "\n",
    "date_encoder = FunctionTransformer(encode_dates, validate=False)\n",
    "date_cols = encode_dates(X[[\"date\"]]).columns.tolist()\n",
    "\n",
    "\n",
    "vacation_paris_2020 = [\n",
    "    (\"2020-10-17\", \"2020-11-01\"),  \n",
    "    (\"2020-12-19\", \"2021-01-03\"),  \n",
    "    (\"2021-02-13\", \"2021-02-28\"),  \n",
    "    (\"2021-04-17\", \"2021-05-02\"), \n",
    "    (\"2021-07-06\", \"2021-08-31\"), \n",
    "]\n",
    "\n",
    "vacation_paris_2020 = [(pd.to_datetime(start), pd.to_datetime(end)) for start, end in vacation_paris_2020]\n",
    "\n",
    "def encode_vacation(X, vacation=vacation_paris_2020):\n",
    "    X = X.copy()\n",
    "    X[\"is_vacation\"] = 0\n",
    "    for start, end in vacation:\n",
    "        X[\"is_vacation\"] |= (X[\"date\"] >= start) & (X[\"date\"] <= end)\n",
    "    X[\"is_vacation\"] = X[\"is_vacation\"].astype(int)\n",
    "\n",
    "    return X\n",
    "\n",
    "vacation_encoder = FunctionTransformer(encode_vacation, validate=False)\n",
    "\n",
    "\n",
    "holidays = [\n",
    "    \"2020-11-01\",  \n",
    "    \"2020-11-11\", \n",
    "    \"2020-12-25\",  \n",
    "    \"2021-01-01\",  \n",
    "    \"2021-04-05\",  \n",
    "    \"2021-05-01\",  \n",
    "    \"2021-05-08\",  \n",
    "    \"2021-05-13\", \n",
    "    \"2021-05-24\",  \n",
    "    \"2021-07-14\",  \n",
    "    \"2021-08-15\", \n",
    "]\n",
    "holidays = pd.to_datetime(holidays)\n",
    "\n",
    "\n",
    "def encode_holidays(X, holidays=holidays):\n",
    "    X = X.copy()\n",
    "    X[\"jour_ferie\"] = X[\"date\"].apply(lambda x: 1 if x in holidays else 0)\n",
    "    return X\n",
    "\n",
    "holidays_encoder = FunctionTransformer(encode_holidays, validate=False)\n",
    "\n",
    "categorical_columns = drop_columns(X).select_dtypes(include=\"category\").columns.to_list()\n",
    "categorical_columns.append(\"site_id\")\n",
    "numerical_columns = drop_columns(X).select_dtypes(include=\"float\").columns.to_list()\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def compute_score(estimator, X_train, X_valid, y_train, y_valid):\n",
    "    estimator.fit(X_train, y_train)\n",
    "    score = root_mean_squared_error(y_valid, estimator.predict(X_valid))\n",
    "    return score\n",
    "\n",
    "\n",
    "weather = pd.read_csv(\"../data/H_75_previous-2020-2022.csv.gz\", sep=\";\",\n",
    "                      parse_dates=[\"AAAAMMJJHH\"],\n",
    "                      date_format=\"%Y%m%d%H\",\n",
    "                      ).rename(columns={\"AAAAMMJJHH\" : \"date\"})\n",
    "\n",
    "\n",
    "weather_columns = [\"U\", \"FF\", \"RR1\", \"T\"]\n",
    "columns_to_merge = weather_columns + [\"date\"]\n",
    "\n",
    "def merge_weather(X, weather=weather, columns_to_merge=columns_to_merge):\n",
    "    weather = weather.copy()\n",
    "    weather = weather[columns_to_merge].sort_values(\"date\")\n",
    "    weather[\"date\"] = weather[\"date\"].astype(\"datetime64[us]\")\n",
    "\n",
    "    start = pd.Timestamp(min(X[\"date\"]))\n",
    "    end = pd.Timestamp(max(X[\"date\"]))\n",
    "\n",
    "    weather = weather[(weather[\"date\"] >= start) & (weather[\"date\"] <= end)]\n",
    "\n",
    "    grouped_weather = weather.groupby(\"date\").mean()\n",
    "\n",
    "    X = X.copy()\n",
    "    initial_index = X.index  # Save original index\n",
    "    X = X.sort_values(\"date\")\n",
    "\n",
    "    # Perform the merge_asof\n",
    "    merged_df = pd.merge_asof(X, grouped_weather, on=\"date\", direction=\"nearest\")\n",
    "\n",
    "    # Reindex to ensure alignment with X\n",
    "    merged_df.index = initial_index  # Reassign original index\n",
    "    merged_df = merged_df.reindex(initial_index)  # Align rows to original order\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "weather_merger = FunctionTransformer(merge_weather, validate=False)\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"date\", OneHotEncoder(handle_unknown=\"ignore\"), date_cols),\n",
    "        # (\"weather numerical\", StandardScaler(), numerical_weather_columns),\n",
    "        # (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"dropper\", column_dropper),\n",
    "        (\"vacation\", vacation_encoder),\n",
    "        (\"date\", date_encoder),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"linear_regressor\", XGBRegressor(n_estimators=350, \n",
    "                                          learning_rate=0.1, \n",
    "                                          max_depth=20, \n",
    "                                          random_state=42, \n",
    "                                          objective='reg:squarederror'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "compute_score(pipeline, X_train, X_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_prediction(model, file_path, test_set=test):\n",
    "    \"\"\"\n",
    "    Return a .csv file for kaggle submission (predictions of the test dataset)\n",
    "    Parameters:\n",
    "        - model : a fitted sklearn model object\n",
    "        - test_set : the dataset to predict log_bike_count on \n",
    "    Output:\n",
    "        - file.csv : a .csv file to submit to kaggle \n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = model.predict(test_set)\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=[\"log_bike_count\"])\n",
    "    y_pred_df.index.name = \"Id\"\n",
    "\n",
    "    y_pred_df.to_csv(file_path)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_prediction(pipeline.fit(X, y), \"../prediction_csvs/test_kaggle.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_classes-9Axu7pbz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
